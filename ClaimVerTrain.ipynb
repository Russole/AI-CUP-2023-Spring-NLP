{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    get_scheduler,\n",
    "    BertTokenizerFast,\n",
    ")\n",
    "\n",
    "from dataset import BERTDataset\n",
    "from utils import (\n",
    "    generate_evidence_to_wiki_pages_mapping,\n",
    "    jsonl_dir_to_df,\n",
    "    load_json,\n",
    "    load_model,\n",
    "    save_checkpoint,\n",
    "    set_lr_scheduler,\n",
    ")\n",
    "\n",
    "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL2ID: Dict[str, int] = {\n",
    "    \"supports\": 0,\n",
    "    \"refutes\": 1,\n",
    "    \"NOT ENOUGH INFO\": 2,\n",
    "}\n",
    "ID2LABEL: Dict[int, str] = {v: k for k, v in LABEL2ID.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TRAIN_DATA = load_json(\"data/Stage2/PublicTrain/WithoutBM25/UnBM25_train_doc10sent5.jsonl\")\n",
    "DEV_DATA = load_json(\"data/Stage2/PublicTrain/WithoutBM25/UnBM25_dev_doc10sent5.jsonl\")\n",
    "\n",
    "TRAIN_PKL_FILE = Path(\"data/Stage2/PublicTrain/WithoutBM25/UnBM25_train_doc10sent5.pkl\")\n",
    "DEV_PKL_FILE = Path(\"data/Stage2/PublicTrain/WithoutBM25/UnBM25_dev_doc10sent5.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and concatenating jsonl files in data/wiki-pages\n",
      "Generate parse mapping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a5757648d83491aa661e0fa885da63d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=296938), Label(value='0 / 296938')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform to id to evidence_map mapping\n"
     ]
    }
   ],
   "source": [
    "wiki_pages = jsonl_dir_to_df(\"data/wiki-pages\")\n",
    "mapping = generate_evidence_to_wiki_pages_mapping(wiki_pages,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AicupTopkEvidenceBERTDataset(BERTDataset):\n",
    "    \"\"\"AICUP dataset with top-k evidence sentences.\"\"\"\n",
    "    \n",
    "    def __getitem__(\n",
    "        self,\n",
    "        idx: int,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[Dict[str, torch.Tensor], int]:\n",
    "        item = self.data.iloc[idx]\n",
    "        claim = item[\"claim\"]\n",
    "        \n",
    "        \n",
    "        evidence=item[\"SingleEvidence\"]\n",
    "        \n",
    "        \n",
    "        \n",
    "        concat = self.tokenizer(\n",
    "            claim,evidence,\n",
    "            padding=\"max_length\",\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "        )\n",
    "        label = LABEL2ID[item[\"label\"]] if \"label\" in item else -1\n",
    "        concat_ten = {k: torch.tensor(v) for k, v in concat.items()}\n",
    "\n",
    "        if \"label\" in item:\n",
    "            concat_ten[\"labels\"] = torch.tensor(label)\n",
    "\n",
    "        return concat_ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AicupTopkEvidenceBERT_Val_Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"AicupTopkEvidenceBERTDataset class for AICUP dataset with top-k evidence sentences.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        tokenizer: AutoTokenizer,\n",
    "        max_length: int = 32,\n",
    "        topk: int = 5,\n",
    "        # TrainMode : bool=True\n",
    "    ):\n",
    "        self.data=data\n",
    "        self.tokenizer=tokenizer\n",
    "        self.max_length=max_length\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(\n",
    "        self,\n",
    "        idx: int,\n",
    "        **kwargs,\n",
    "    ) :\n",
    "        item = self.data.iloc[idx]\n",
    "        sentA = item[\"claim\"]\n",
    "        predicted_pages_text={}\n",
    "        sentB = item[\"evidence_list\"]\n",
    "        label = LABEL2ID[item[\"label\"]] if \"label\" in item else -1\n",
    "        \n",
    "        for num,text in enumerate(sentB):\n",
    "            PredictedTextToken = self.tokenizer(\n",
    "            sentA,text,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            )\n",
    "            CandidateNum=f\"ClaimEvidence{num+1}\"\n",
    "            predicted_pages_text[CandidateNum]=PredictedTextToken\n",
    "        \n",
    "        \n",
    "        for key,value in predicted_pages_text.items():\n",
    "            predicted_pages_text[key]={k: torch.tensor(v) for k, v in predicted_pages_text[key].items()}\n",
    "        label=torch.tensor(label)\n",
    "        predicted_pages_text[\"labels\"]=label\n",
    "        \n",
    "        return predicted_pages_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RuleBasedAccuracy(y_pred,y_pred2,y_pred3,y_pred4,y_pred5):\n",
    "    final_pred=[]\n",
    "    \n",
    "    for num,i in enumerate (y_pred):\n",
    "        if(i==0 or y_pred2[num]==0 or y_pred3[num]==0 or y_pred4[num]==0 or y_pred5[num]==0):\n",
    "            final_pred.append(0)\n",
    "        elif(i==1 or y_pred2[num]==1 or y_pred3[num]==1 or y_pred4[num]==1 or y_pred5[num]==1):\n",
    "            final_pred.append(1)\n",
    "        else:\n",
    "            final_pred.append(2)\n",
    "    \n",
    "    return final_pred\n",
    "def run_evaluation(model: torch.nn.Module, dataloader: DataLoader, device):\n",
    "    model.eval()\n",
    "\n",
    "    loss = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_pred2 = []\n",
    "    y_pred3 = []\n",
    "    y_pred4 = []\n",
    "    y_pred5 = []\n",
    "    final_pred=[]\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            y_true.extend(batch[\"labels\"].tolist())\n",
    "            \n",
    "            ####### ClaimEvidence1 #########\n",
    "            batch_2=batch[\"ClaimEvidence1\"]\n",
    "            batch_2[\"labels\"]=batch[\"labels\"]\n",
    "            batch_2 = {k: v.to(device) for k, v in batch_2.items()}\n",
    "            \n",
    "            outputs = model(**batch_2)\n",
    "            ####### ClaimEvidence2 #########\n",
    "            batch_2_2=batch[\"ClaimEvidence2\"]\n",
    "            batch_2_2 = {k: v.to(device) for k, v in batch_2_2.items()}\n",
    "            outputs2 = model(**batch_2_2)\n",
    "            \n",
    "            ####### ClaimEvidence3 #########\n",
    "            batch_2_3=batch[\"ClaimEvidence3\"]\n",
    "            batch_2_3 = {k: v.to(device) for k, v in batch_2_3.items()}\n",
    "            outputs3 = model(**batch_2_3)\n",
    "            \n",
    "            ####### ClaimEvidence4 #########\n",
    "            batch_2_4=batch[\"ClaimEvidence4\"]\n",
    "            batch_2_4 = {k: v.to(device) for k, v in batch_2_4.items()}\n",
    "            outputs4 = model(**batch_2_4)\n",
    "\n",
    "            ####### ClaimEvidence5 #########\n",
    "            batch_2_5=batch[\"ClaimEvidence5\"]\n",
    "            batch_2_5 = {k: v.to(device) for k, v in batch_2_5.items()}\n",
    "            outputs5 = model(**batch_2_5)\n",
    "\n",
    "\n",
    "\n",
    "            #print(outputs.logits.shape)\n",
    "            loss += outputs.loss.item() #outputs loss\n",
    "            logits = outputs.logits#logits shape:torch.Size([32, 3])\n",
    "            logits2 = outputs2.logits\n",
    "            logits3 = outputs3.logits\n",
    "            logits4 = outputs4.logits\n",
    "            logits5 = outputs5.logits\n",
    "\n",
    "\n",
    "            y_pred.extend(torch.argmax(logits, dim=1).tolist())\n",
    "            y_pred2.extend(torch.argmax(logits2, dim=1).tolist())\n",
    "            y_pred3.extend(torch.argmax(logits3, dim=1).tolist())\n",
    "            y_pred4.extend(torch.argmax(logits4, dim=1).tolist())\n",
    "            y_pred5.extend(torch.argmax(logits5, dim=1).tolist())\n",
    "            \n",
    "        final_pred=RuleBasedAccuracy(y_pred,y_pred2,y_pred3,y_pred4,y_pred5)\n",
    "    \n",
    "    # acc = accuracy_score(y_true, y_pred)\n",
    "    acc = accuracy_score(y_true, final_pred)\n",
    "\n",
    "    return {\"val_loss\": loss / len(dataloader), \"val_acc\": acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_with_topk_evidence(\n",
    "    df: pd.DataFrame,\n",
    "    mapping: dict,\n",
    "    mode: str = \"train\",\n",
    "    topk: int = 5,\n",
    "    train_evidence_split=True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"join_with_topk_evidence join the dataset with topk evidence.\n",
    "\n",
    "    Note:\n",
    "        After extraction, the dataset will be like this:\n",
    "               id     label         claim                           evidence            evidence_list\n",
    "        0    4604  supports       高行健...     [[[3393, 3552, 高行健, 0], [...  [高行健 （ ）江西赣州出...\n",
    "        ..    ...       ...            ...                                ...                     ...\n",
    "        945  2095  supports       美國總...  [[[1879, 2032, 吉米·卡特, 16], [...  [卸任后 ， 卡特積極參與...\n",
    "        停各种战争及人質危機的斡旋工作 ， 反对美国小布什政府攻打伊拉克...\n",
    "\n",
    "        [946 rows x 5 columns]\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataset with evidence.\n",
    "        wiki_pages (pd.DataFrame): The wiki pages dataframe\n",
    "        topk (int, optional): The topk evidence. Defaults to 5.\n",
    "        cache(Union[Path, str], optional): The cache file path. Defaults to None.\n",
    "            If cache is None, return the result directly.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The dataset with topk evidence_list.\n",
    "            The `evidence_list` column will be: List[str]\n",
    "    \"\"\"\n",
    "\n",
    "    # format evidence column to List[List[Tuple[str, str, str, str]]]\n",
    "    if \"evidence\" in df.columns:\n",
    "        df[\"evidence\"] = df[\"evidence\"].parallel_map(\n",
    "            lambda x: [[x]] if not isinstance(x[0], list) else [x]\n",
    "            if not isinstance(x[0][0], list) else x)\n",
    "\n",
    "    print(f\"Extracting evidence_list for the {mode} mode ...\")\n",
    "    if mode == \"eval\":\n",
    "        # extract evidence\n",
    "        df[\"evidence_list\"] = df[\"predicted_evidence\"].parallel_map(lambda x: [\n",
    "            mapping.get(evi_id, {}).get(str(evi_idx), \"\")\n",
    "            for evi_id, evi_idx in x  # for each evidence list\n",
    "        ][:topk] if isinstance(x, list) else [])\n",
    "        print(df[\"evidence_list\"][:5])\n",
    "    else:\n",
    "        \n",
    "\n",
    "        \n",
    "        # extract evidence\n",
    "        df[\"evidence_list\"] = df[\"evidence\"].parallel_map(lambda x: [\n",
    "            \" \".join([  # join evidence\n",
    "                mapping.get(evi_id, {}).get(str(evi_idx), \"\")\n",
    "                for _, _, evi_id, evi_idx in evi_list\n",
    "            ]) if isinstance(evi_list, list) else \"\"\n",
    "            for evi_list in x  # for each evidence list\n",
    "            ][:len(x)] if isinstance(x, list) else [])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title  { display-mode: \"form\" }\n",
    "\n",
    "#MODEL_NAME = \"bert-base-chinese\"  #@param {type:\"string\"} #ckiplab/\n",
    "MODEL_NAME = \"ckiplab/bert-base-chinese\"\n",
    "#MODEL_NAME = \"ckiplab/albert-base-chinese\" #albert-base-chinese\n",
    "TRAIN_BATCH_SIZE = 32  #@param {type:\"integer\"}\n",
    "TEST_BATCH_SIZE = 32  #@param {type:\"integer\"}\n",
    "SEED = 42  #@param {type:\"integer\"}\n",
    "LR = 7e-5  #@param {type:\"number\"}\n",
    "NUM_EPOCHS = 20  #@param {type:\"integer\"}\n",
    "MAX_SEQ_LEN = 256  #@param {type:\"integer\"}\n",
    "# EVIDENCE_TOPK = 5  #@param {type:\"integer\"} #default parameter\n",
    "EVIDENCE_TOPK = 5\n",
    "#EVIDENCE_TOPK = 1\n",
    "VALIDATION_STEP = 1000  #@param {type:\"integer\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILENAME = \"submission.jsonl\"\n",
    "\n",
    "EXP_DIR = f\"/第二階段資料/無BM25/claim_verification/e{NUM_EPOCHS}_bs{TRAIN_BATCH_SIZE}_\" + f\"{LR}_top{EVIDENCE_TOPK}\"\n",
    "LOG_DIR = \"logs/\" + EXP_DIR\n",
    "CKPT_DIR = \"checkpoints/\" + EXP_DIR\n",
    "\n",
    "if not Path(LOG_DIR).exists():\n",
    "    Path(LOG_DIR).mkdir(parents=True)\n",
    "\n",
    "if not Path(CKPT_DIR).exists():\n",
    "    Path(CKPT_DIR).mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TRAIN_PKL_FILE.exists():\n",
    "    train_df = join_with_topk_evidence(\n",
    "        pd.DataFrame(TRAIN_DATA),\n",
    "        mapping,\n",
    "        topk=EVIDENCE_TOPK,\n",
    "    )\n",
    "    train_df.to_pickle(TRAIN_PKL_FILE, protocol=4)\n",
    "else:\n",
    "    with open(TRAIN_PKL_FILE, \"rb\") as f:\n",
    "        train_df = pickle.load(f) #原本\n",
    "        #train_df = pd.read_pickle(f) \n",
    "\n",
    "if not DEV_PKL_FILE.exists():\n",
    "    dev_df = join_with_topk_evidence(\n",
    "        pd.DataFrame(DEV_DATA),\n",
    "        mapping,\n",
    "        mode=\"eval\",\n",
    "        topk=EVIDENCE_TOPK,\n",
    "    )\n",
    "    dev_df.to_pickle(DEV_PKL_FILE, protocol=4)\n",
    "else:\n",
    "    with open(DEV_PKL_FILE, \"rb\") as f:\n",
    "        dev_df = pickle.load(f) #原本 read_pickle\n",
    "        #dev_df = pd.read_pickle(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_short=dev_df[dev_df[\"predicted_evidence\"].map(len)<5]#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df=dev_df.drop(dev_df[dev_df[\"predicted_evidence\"].map(len)<5].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOT_ENOUGH_INFO_ROW=train_df[train_df[\"predicted_evidence\"].map(len)==0]#predicted_evidence=[], NOT_ENOUGH_INFO_ROW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=train_df.drop(train_df[train_df[\"predicted_evidence\"].map(len)==0].index)#drop predicted_evidence=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "def RandomPredictedEvidence(df):\n",
    "    \n",
    "    Max=len(df[\"predicted_evidence\"])-1\n",
    "    if Max != -1:\n",
    "        \n",
    "        if Max!=0:\n",
    "            RandomEviList=[]\n",
    "            if Max==1:\n",
    "                RandomIndex=random.sample(range(0,Max+1),2)#Sample 2\n",
    "                for index in RandomIndex:\n",
    "                    RandomEvi=df[\"predicted_evidence\"][index]\n",
    "                    RandomEviList.append(mapping[RandomEvi[0]][str(RandomEvi[1])])\n",
    "        \n",
    "                return RandomEviList\n",
    "            else:\n",
    "                RandomIndex=random.sample(range(0,Max+1),3)#Sample 3\n",
    "                for index in RandomIndex:\n",
    "                    RandomEvi=df[\"predicted_evidence\"][index]\n",
    "                    RandomEviList.append(mapping[RandomEvi[0]][str(RandomEvi[1])])\n",
    "        \n",
    "                return RandomEviList\n",
    "        else:\n",
    "            RandomEvi=df[\"predicted_evidence\"][0]\n",
    "            return mapping[RandomEvi[0]][str(RandomEvi[1])]\n",
    "    \n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DropNotInfo=False\n",
    "if DropNotInfo:\n",
    "    train_df=train_df.drop(train_df[train_df[\"label\"]==\"NOT ENOUGH INFO\"].index)\n",
    "    dev_df=dev_df.drop(dev_df[dev_df[\"label\"]==\"NOT ENOUGH INFO\"].index)\n",
    "else:\n",
    "    a=train_df[train_df[\"label\"]==\"NOT ENOUGH INFO\"].index\n",
    "    a=list(a)\n",
    "    train_df.loc[a,\"evidence_list\"]=train_df[train_df[\"label\"]==\"NOT ENOUGH INFO\"].apply(RandomPredictedEvidence,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=train_df.explode('evidence_list')\n",
    "train_df=train_df.rename(columns={\"evidence_list\": \"SingleEvidence\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-chinese\")#BertTokenizerFast\n",
    "train_dataset = AicupTopkEvidenceBERTDataset(\n",
    "    train_df,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_SEQ_LEN,\n",
    "    topk=EVIDENCE_TOPK\n",
    ")\n",
    "val_dataset = AicupTopkEvidenceBERT_Val_Dataset(\n",
    "    dev_df,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_SEQ_LEN,\n",
    "    topk=EVIDENCE_TOPK\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    ")\n",
    "eval_dataloader = DataLoader(val_dataset, batch_size=TEST_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ckiplab/bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ckiplab/bert-base-chinese and are newly initialized: ['classifier.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\n",
    "    \"cpu\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(LABEL2ID),\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LR)\n",
    "num_training_steps = NUM_EPOCHS * len(train_dataloader)\n",
    "lr_scheduler = set_lr_scheduler(optimizer, num_training_steps)\n",
    "\n",
    "writer = SummaryWriter(LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "current_steps = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    y_true_list = []\n",
    "    y_pred_list = []\n",
    "    Trainloss=0\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        Trainloss+=loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "        writer.add_scalar(\"training_loss\", loss.item(), current_steps)\n",
    "\n",
    "        y_pred = torch.argmax(outputs.logits, dim=1).tolist()\n",
    "        y_true = batch[\"labels\"].tolist()\n",
    "        \n",
    "        y_pred_list.extend(y_pred)\n",
    "        y_true_list.extend(y_true)\n",
    "\n",
    "        current_steps += 1\n",
    "\n",
    "        if current_steps % VALIDATION_STEP == 0 and current_steps > 0:\n",
    "            print(\"Start validation\")\n",
    "            val_results = run_evaluation(model, eval_dataloader, device)\n",
    "\n",
    "            # log each metric separately to TensorBoard\n",
    "            for metric_name, metric_value in val_results.items():\n",
    "                print(f\"{metric_name}: {metric_value}\")\n",
    "                writer.add_scalar(f\"{metric_name}\", metric_value, current_steps)\n",
    "\n",
    "            save_checkpoint(\n",
    "                model,\n",
    "                CKPT_DIR,\n",
    "                current_steps,\n",
    "                mark=f\"val_acc={val_results['val_acc']:.4f}\",\n",
    "            )\n",
    "    \n",
    "    train_acc = accuracy_score(y_true_list, y_pred_list)\n",
    "    print(f\"train_loss: {Trainloss / len(train_dataloader)}, train_acc:{train_acc}\")\n",
    "    \n",
    "    \n",
    "\n",
    "print(\"Finished training!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Not Enough Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NotEnoughInfoDev=dev_df[dev_df[\"label\"]==\"NOT ENOUGH INFO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset_not_info = AicupTopkEvidenceBERT_Val_Dataset(\n",
    "    NotEnoughInfoDev,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_SEQ_LEN,\n",
    "    topk=EVIDENCE_TOPK\n",
    ")\n",
    "valid_dataloader_not_info = DataLoader(valid_dataset_not_info, batch_size=5)\n",
    "run_evaluation(model, valid_dataloader_not_info, device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Enough Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EnoughInfoDev=dev_df[dev_df[\"label\"]!=\"NOT ENOUGH INFO\"]\n",
    "valid_dataset_info = AicupTopkEvidenceBERT_Val_Dataset(\n",
    "    EnoughInfoDev,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_SEQ_LEN,\n",
    "    topk=EVIDENCE_TOPK\n",
    ")\n",
    "valid_dataloader_info = DataLoader(valid_dataset_info, batch_size=5)\n",
    "run_evaluation(model, valid_dataloader_info, device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Full Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "valid_dataset_full = AicupTopkEvidenceBERT_Val_Dataset(\n",
    "    dev_df,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_SEQ_LEN,\n",
    "    topk=EVIDENCE_TOPK\n",
    ")\n",
    "valid_dataloader_full = DataLoader(valid_dataset_full, batch_size=5)\n",
    "run_evaluation(model, valid_dataloader_full, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
