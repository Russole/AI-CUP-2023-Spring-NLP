{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    get_scheduler,\n",
    "    BertTokenizerFast,\n",
    ")\n",
    "\n",
    "from dataset import BERTDataset\n",
    "from utils import (\n",
    "    generate_evidence_to_wiki_pages_mapping,\n",
    "    jsonl_dir_to_df,\n",
    "    load_json,\n",
    "    load_model,\n",
    "    save_checkpoint,\n",
    "    set_lr_scheduler,\n",
    ")\n",
    "\n",
    "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL2ID: Dict[str, int] = {\n",
    "    \"supports\": 0,\n",
    "    \"refutes\": 1,\n",
    "    \"NOT ENOUGH INFO\": 2,\n",
    "}\n",
    "ID2LABEL: Dict[int, str] = {v: k for k, v in LABEL2ID.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and concatenating jsonl files in data/wiki-pages\n",
      "Generate parse mapping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad76b05f2e124e64b0b2acff6edefb4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=296938), Label(value='0 / 296938')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform to id to evidence_map mapping\n"
     ]
    }
   ],
   "source": [
    "wiki_pages = jsonl_dir_to_df(\"data/wiki-pages\")\n",
    "mapping = generate_evidence_to_wiki_pages_mapping(wiki_pages,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AicupTopkEvidenceBERT_Test_Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"AicupTopkEvidenceBERTDataset class for AICUP dataset with top-k evidence sentences.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        tokenizer: AutoTokenizer,\n",
    "        max_length: int = 32,\n",
    "        topk: int = 5,\n",
    "        # TrainMode : bool=True\n",
    "    ):\n",
    "        self.data=data\n",
    "        self.tokenizer=tokenizer\n",
    "        self.max_length=max_length\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(\n",
    "        self,\n",
    "        idx: int,\n",
    "        **kwargs,\n",
    "    ) :\n",
    "        item = self.data.iloc[idx]\n",
    "        sentA = item[\"claim\"]\n",
    "        predicted_pages_text={}\n",
    "        sentB = item[\"evidence_list\"]\n",
    "        \n",
    "        for num,text in enumerate(sentB):\n",
    "            PredictedTextToken = self.tokenizer(\n",
    "            sentA,text,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            )\n",
    "            CandidateNum=f\"ClaimEvidence{num+1}\"\n",
    "            predicted_pages_text[CandidateNum]=PredictedTextToken\n",
    "        \n",
    "        \n",
    "        for key,value in predicted_pages_text.items():\n",
    "            predicted_pages_text[key]={k: torch.tensor(v) for k, v in predicted_pages_text[key].items()}\n",
    "        \n",
    "        return predicted_pages_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RuleBasedAccuracy(y_pred,y_pred2,y_pred3,y_pred4,y_pred5):\n",
    "    final_pred=[]\n",
    "    \n",
    "    for num,i in enumerate (y_pred):\n",
    "        if(i==0 or y_pred2[num]==0 or y_pred3[num]==0 or y_pred4[num]==0 or y_pred5[num]==0):\n",
    "            final_pred.append(0)\n",
    "        elif(i==1 or y_pred2[num]==1 or y_pred3[num]==1 or y_pred4[num]==1 or y_pred5[num]==1):\n",
    "            final_pred.append(1)\n",
    "        else:\n",
    "            final_pred.append(2)\n",
    "    \n",
    "    return final_pred\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RuleBasedAccuracy_1(y_pred):\n",
    "    final_pred=[]\n",
    "    \n",
    "    for num,i in enumerate (y_pred):\n",
    "        if(i==0  ):\n",
    "            final_pred.append(0)\n",
    "        elif(i==1 ):\n",
    "            final_pred.append(1)\n",
    "        else:\n",
    "            final_pred.append(2)\n",
    "    \n",
    "    return final_pred\n",
    "def RuleBasedAccuracy_2(y_pred,y_pred2):\n",
    "    final_pred=[]\n",
    "    \n",
    "    for num,i in enumerate (y_pred):\n",
    "        if(i==0 or y_pred2[num]==0 ):\n",
    "            final_pred.append(0)\n",
    "        elif(i==1 or y_pred2[num]==1 ):\n",
    "            final_pred.append(1)\n",
    "        else:\n",
    "            final_pred.append(2)\n",
    "    \n",
    "    return final_pred\n",
    "        \n",
    "def RuleBasedAccuracy_3(y_pred,y_pred2,y_pred3):\n",
    "    final_pred=[]\n",
    "    \n",
    "    for num,i in enumerate (y_pred):\n",
    "        if(i==0 or y_pred2[num]==0 or y_pred3[num]==0):\n",
    "            final_pred.append(0)\n",
    "        elif(i==1 or y_pred2[num]==1 or y_pred2[num]==1):\n",
    "            final_pred.append(1)\n",
    "        else:\n",
    "            final_pred.append(2)\n",
    "    \n",
    "    return final_pred\n",
    "\n",
    "def RuleBasedAccuracy_4(y_pred,y_pred2,y_pred3,y_pred4):\n",
    "    final_pred=[]\n",
    "    \n",
    "    for num,i in enumerate (y_pred):\n",
    "        if(i==0 or y_pred2[num]==0 or y_pred3[num]==0 or y_pred4[num]==0 ):\n",
    "            final_pred.append(0)\n",
    "        elif(i==1 or y_pred2[num]==1 or y_pred3[num]==1 or y_pred4[num]==1 ):\n",
    "            final_pred.append(1)\n",
    "        else:\n",
    "            final_pred.append(2)\n",
    "    \n",
    "    return final_pred\n",
    "        \n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_predict(model: torch.nn.Module, test_dl: DataLoader, device) -> list:\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_pred2 = []\n",
    "    y_pred3 = []\n",
    "    y_pred4 = []\n",
    "    y_pred5 = []\n",
    "    \n",
    "    count=0\n",
    "    Prediction={}\n",
    "    for batch in tqdm(test_dl,\n",
    "                      total=len(test_dl),\n",
    "                      leave=False,\n",
    "                      desc=\"Predicting\"):\n",
    "        \n",
    "\n",
    "        ####### ClaimEvidence1 #########\n",
    "        batch_2=batch[\"ClaimEvidence1\"]\n",
    "            #batch_2[\"labels\"]=batch[\"labels\"]\n",
    "        batch_2 = {k: v.to(device) for k, v in batch_2.items()}\n",
    "            \n",
    "        outputs = model(**batch_2)\n",
    "        ####### ClaimEvidence2 #########\n",
    "        batch_2_2=batch[\"ClaimEvidence2\"]\n",
    "        batch_2_2 = {k: v.to(device) for k, v in batch_2_2.items()}\n",
    "        outputs2 = model(**batch_2_2)\n",
    "            \n",
    "        ####### ClaimEvidence3 #########\n",
    "        batch_2_3=batch[\"ClaimEvidence3\"]\n",
    "        batch_2_3 = {k: v.to(device) for k, v in batch_2_3.items()}\n",
    "        outputs3 = model(**batch_2_3)\n",
    "            \n",
    "        ####### ClaimEvidence4 #########\n",
    "        batch_2_4=batch[\"ClaimEvidence4\"]\n",
    "        batch_2_4 = {k: v.to(device) for k, v in batch_2_4.items()}\n",
    "        outputs4 = model(**batch_2_4)\n",
    "\n",
    "        ####### ClaimEvidence5 #########\n",
    "        batch_2_5=batch[\"ClaimEvidence5\"]\n",
    "        batch_2_5 = {k: v.to(device) for k, v in batch_2_5.items()}\n",
    "        outputs5 = model(**batch_2_5)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        logits = outputs.logits#logits shape:torch.Size([32, 3])\n",
    "        logits2 = outputs2.logits\n",
    "        logits3 = outputs3.logits\n",
    "        logits4 = outputs4.logits\n",
    "        logits5 = outputs5.logits\n",
    "\n",
    "\n",
    "        y_pred.extend(torch.argmax(logits, dim=1).tolist())\n",
    "        y_pred2.extend(torch.argmax(logits2, dim=1).tolist())\n",
    "        y_pred3.extend(torch.argmax(logits3, dim=1).tolist())\n",
    "        y_pred4.extend(torch.argmax(logits4, dim=1).tolist())\n",
    "        y_pred5.extend(torch.argmax(logits5, dim=1).tolist())\n",
    "        \n",
    "\n",
    "            \n",
    "    preds=RuleBasedAccuracy(y_pred,y_pred2,y_pred3,y_pred4,y_pred5)\n",
    "\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_predict_1(model: torch.nn.Module, test_dl: DataLoader, device) -> list:\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_pred2 = []\n",
    "    y_pred3 = []\n",
    "    y_pred4 = []\n",
    "    y_pred5 = []\n",
    "    \n",
    "    count=0\n",
    "    Prediction={}\n",
    "    for batch in tqdm(test_dl,\n",
    "                      total=len(test_dl),\n",
    "                      leave=False,\n",
    "                      desc=\"Predicting\"):\n",
    "        \n",
    "        ####### ClaimEvidence1 #########\n",
    "        batch_2=batch[\"ClaimEvidence1\"]\n",
    "            #batch_2[\"labels\"]=batch[\"labels\"]\n",
    "        batch_2 = {k: v.to(device) for k, v in batch_2.items()}\n",
    "            \n",
    "        outputs = model(**batch_2)\n",
    "        \n",
    "        logits = outputs.logits#logits shape:torch.Size([32, 3])\n",
    "      \n",
    "\n",
    "\n",
    "        y_pred.extend(torch.argmax(logits, dim=1).tolist())\n",
    "        \n",
    "\n",
    "            \n",
    "    preds=RuleBasedAccuracy_1(y_pred)\n",
    "\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_predict_2(model: torch.nn.Module, test_dl: DataLoader, device) -> list:\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_pred2 = []\n",
    "    y_pred3 = []\n",
    "    y_pred4 = []\n",
    "    y_pred5 = []\n",
    "    \n",
    "    count=0\n",
    "    Prediction={}\n",
    "    for batch in tqdm(test_dl,\n",
    "                      total=len(test_dl),\n",
    "                      leave=False,\n",
    "                      desc=\"Predicting\"):\n",
    "        \n",
    "        ####### ClaimEvidence1 #########\n",
    "        batch_2=batch[\"ClaimEvidence1\"]\n",
    "            #batch_2[\"labels\"]=batch[\"labels\"]\n",
    "        batch_2 = {k: v.to(device) for k, v in batch_2.items()}\n",
    "            \n",
    "        outputs = model(**batch_2)\n",
    "        ####### ClaimEvidence2 #########\n",
    "        batch_2_2=batch[\"ClaimEvidence2\"]\n",
    "        batch_2_2 = {k: v.to(device) for k, v in batch_2_2.items()}\n",
    "        outputs2 = model(**batch_2_2)\n",
    "            \n",
    "       \n",
    "        logits = outputs.logits#logits shape:torch.Size([32, 3])\n",
    "        logits2 = outputs2.logits\n",
    "        \n",
    "\n",
    "\n",
    "        y_pred.extend(torch.argmax(logits, dim=1).tolist())\n",
    "        y_pred2.extend(torch.argmax(logits2, dim=1).tolist())\n",
    "        \n",
    "\n",
    "            \n",
    "    preds=RuleBasedAccuracy_2(y_pred,y_pred2)\n",
    "\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_predict_3(model: torch.nn.Module, test_dl: DataLoader, device) -> list:\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_pred2 = []\n",
    "    y_pred3 = []\n",
    "    y_pred4 = []\n",
    "    y_pred5 = []\n",
    "    \n",
    "    count=0\n",
    "    Prediction={}\n",
    "    for batch in tqdm(test_dl,\n",
    "                      total=len(test_dl),\n",
    "                      leave=False,\n",
    "                      desc=\"Predicting\"):\n",
    "       \n",
    "\n",
    "        ####### ClaimEvidence1 #########\n",
    "        batch_2=batch[\"ClaimEvidence1\"]\n",
    "            #batch_2[\"labels\"]=batch[\"labels\"]\n",
    "        batch_2 = {k: v.to(device) for k, v in batch_2.items()}\n",
    "            \n",
    "        outputs = model(**batch_2)\n",
    "        ####### ClaimEvidence2 #########\n",
    "        batch_2_2=batch[\"ClaimEvidence2\"]\n",
    "        batch_2_2 = {k: v.to(device) for k, v in batch_2_2.items()}\n",
    "        outputs2 = model(**batch_2_2)\n",
    "            \n",
    "        ####### ClaimEvidence3 #########\n",
    "        batch_2_3=batch[\"ClaimEvidence3\"]\n",
    "        batch_2_3 = {k: v.to(device) for k, v in batch_2_3.items()}\n",
    "        outputs3 = model(**batch_2_3)\n",
    "            \n",
    "        \n",
    "        logits = outputs.logits#logits shape:torch.Size([32, 3])\n",
    "        logits2 = outputs2.logits\n",
    "        logits3 = outputs3.logits\n",
    "        \n",
    "\n",
    "\n",
    "        y_pred.extend(torch.argmax(logits, dim=1).tolist())\n",
    "        y_pred2.extend(torch.argmax(logits2, dim=1).tolist())\n",
    "        y_pred3.extend(torch.argmax(logits3, dim=1).tolist())\n",
    "        \n",
    "\n",
    "            \n",
    "    preds=RuleBasedAccuracy_3(y_pred,y_pred2,y_pred3)\n",
    "\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_predict_4(model: torch.nn.Module, test_dl: DataLoader, device) -> list:\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_pred2 = []\n",
    "    y_pred3 = []\n",
    "    y_pred4 = []\n",
    "    y_pred5 = []\n",
    "    \n",
    "    count=0\n",
    "    Prediction={}\n",
    "    for batch in tqdm(test_dl,\n",
    "                      total=len(test_dl),\n",
    "                      leave=False,\n",
    "                      desc=\"Predicting\"):\n",
    "       \n",
    "\n",
    "        ####### ClaimEvidence1 #########\n",
    "        batch_2=batch[\"ClaimEvidence1\"]\n",
    "            #batch_2[\"labels\"]=batch[\"labels\"]\n",
    "        batch_2 = {k: v.to(device) for k, v in batch_2.items()}\n",
    "            \n",
    "        outputs = model(**batch_2)\n",
    "        ####### ClaimEvidence2 #########\n",
    "        batch_2_2=batch[\"ClaimEvidence2\"]\n",
    "        batch_2_2 = {k: v.to(device) for k, v in batch_2_2.items()}\n",
    "        outputs2 = model(**batch_2_2)\n",
    "            \n",
    "        ####### ClaimEvidence3 #########\n",
    "        batch_2_3=batch[\"ClaimEvidence3\"]\n",
    "        batch_2_3 = {k: v.to(device) for k, v in batch_2_3.items()}\n",
    "        outputs3 = model(**batch_2_3)\n",
    "            \n",
    "        ####### ClaimEvidence4 #########\n",
    "        batch_2_4=batch[\"ClaimEvidence4\"]\n",
    "        batch_2_4 = {k: v.to(device) for k, v in batch_2_4.items()}\n",
    "        outputs4 = model(**batch_2_4)\n",
    "\n",
    "       \n",
    "        logits = outputs.logits#logits shape:torch.Size([32, 3])\n",
    "        logits2 = outputs2.logits\n",
    "        logits3 = outputs3.logits\n",
    "        logits4 = outputs4.logits\n",
    "        # logits5 = outputs5.logits\n",
    "\n",
    "\n",
    "        y_pred.extend(torch.argmax(logits, dim=1).tolist())\n",
    "        y_pred2.extend(torch.argmax(logits2, dim=1).tolist())\n",
    "        y_pred3.extend(torch.argmax(logits3, dim=1).tolist())\n",
    "        y_pred4.extend(torch.argmax(logits4, dim=1).tolist())\n",
    "        \n",
    "\n",
    "            \n",
    "    preds=RuleBasedAccuracy_4(y_pred,y_pred2,y_pred3,y_pred4)\n",
    "\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_with_topk_evidence(\n",
    "    df: pd.DataFrame,\n",
    "    mapping: dict,\n",
    "    mode: str = \"train\",\n",
    "    topk: int = 5,\n",
    "    train_evidence_split=True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"join_with_topk_evidence join the dataset with topk evidence.\n",
    "\n",
    "    Note:\n",
    "        After extraction, the dataset will be like this:\n",
    "               id     label         claim                           evidence            evidence_list\n",
    "        0    4604  supports       高行健...     [[[3393, 3552, 高行健, 0], [...  [高行健 （ ）江西赣州出...\n",
    "        ..    ...       ...            ...                                ...                     ...\n",
    "        945  2095  supports       美國總...  [[[1879, 2032, 吉米·卡特, 16], [...  [卸任后 ， 卡特積極參與...\n",
    "        停各种战争及人質危機的斡旋工作 ， 反对美国小布什政府攻打伊拉克...\n",
    "\n",
    "        [946 rows x 5 columns]\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataset with evidence.\n",
    "        wiki_pages (pd.DataFrame): The wiki pages dataframe\n",
    "        topk (int, optional): The topk evidence. Defaults to 5.\n",
    "        cache(Union[Path, str], optional): The cache file path. Defaults to None.\n",
    "            If cache is None, return the result directly.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The dataset with topk evidence_list.\n",
    "            The `evidence_list` column will be: List[str]\n",
    "    \"\"\"\n",
    "\n",
    "    # format evidence column to List[List[Tuple[str, str, str, str]]]\n",
    "    if \"evidence\" in df.columns:\n",
    "        df[\"evidence\"] = df[\"evidence\"].parallel_map(\n",
    "            lambda x: [[x]] if not isinstance(x[0], list) else [x]\n",
    "            if not isinstance(x[0][0], list) else x)\n",
    "\n",
    "    print(f\"Extracting evidence_list for the {mode} mode ...\")\n",
    "    if mode == \"eval\":\n",
    "        # extract evidence\n",
    "        df[\"evidence_list\"] = df[\"predicted_evidence\"].parallel_map(lambda x: [\n",
    "            mapping.get(evi_id, {}).get(str(evi_idx), \"\")\n",
    "            for evi_id, evi_idx in x  # for each evidence list\n",
    "        ][:topk] if isinstance(x, list) else [])\n",
    "        print(df[\"evidence_list\"][:5])\n",
    "    else:\n",
    "        \n",
    "        # extract evidence\n",
    "        df[\"evidence_list\"] = df[\"evidence\"].parallel_map(lambda x: [\n",
    "            \" \".join([  # join evidence\n",
    "                mapping.get(evi_id, {}).get(str(evi_idx), \"\")\n",
    "                for _, _, evi_id, evi_idx in evi_list\n",
    "            ]) if isinstance(evi_list, list) else \"\"\n",
    "            for evi_list in x  # for each evidence list\n",
    "            ][:len(x)] if isinstance(x, list) else [])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title  { display-mode: \"form\" }\n",
    "\n",
    "MODEL_NAME = \"ckiplab/bert-base-chinese\"\n",
    "TEST_BATCH_SIZE = 32  #@param {type:\"integer\"}\n",
    "MAX_SEQ_LEN = 256  #@param {type:\"integer\"}\n",
    "EVIDENCE_TOPK = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ckiplab/bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ckiplab/bert-base-chinese and are newly initialized: ['bert.pooler.dense.bias', 'classifier.weight', 'bert.pooler.dense.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\n",
    "    \"cpu\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(LABEL2ID),\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Private Data:8049筆\n"
     ]
    }
   ],
   "source": [
    "TEST_DATA = load_json(\"data/Stage2/Private/Private_doc10sent5_BM25_ver1_final.jsonl\")\n",
    "TEST_PKL_FILE = Path(\"data/Stage2/Private/Private_doc10sent5_BM25_ver1_final.pkl\")\n",
    "print(f\"Private Data:{len(TEST_DATA)}筆\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA_Frame=pd.DataFrame(TEST_DATA)\n",
    "TEST_DATA_None=TEST_DATA_Frame[TEST_DATA_Frame[\"predicted_evidence\"].map(len)==0]\n",
    "TEST_DATA_Short=TEST_DATA_Frame[TEST_DATA_Frame[\"predicted_evidence\"].map(len)<5]\n",
    "TEST_DATA_Short=TEST_DATA_Short.drop(TEST_DATA_Short[TEST_DATA_Short[\"predicted_evidence\"].map(len)==0].index)\n",
    "TEST_DATA_Frame=TEST_DATA_Frame.drop(TEST_DATA_Frame[TEST_DATA_Frame[\"predicted_evidence\"].map(len)<5].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting evidence_list for the eval mode ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4371b62e0626438abcbae81d4d4457ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1998), Label(value='0 / 1998'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [這一目中的鳥有些體態雄健優美 ， 色彩豔麗 ， 其中不少是珍稀物種和經濟物種 ， 與人類的...\n",
      "1    [教會剛成立時爲解決教會里發生的一些問題 ， 使徒 （ 宗徒 ） 們寫下許多書信 ， 這些書...\n",
      "2    [1973年3月17日羅伯 · 昆蘭生於明尼蘇達州聖保羅市 。, 1973年3月17日羅伯 ...\n",
      "3    [小威廉絲是上屆的女子單打冠軍 ， 但她在決賽敗給加比涅 · 穆古魯薩 。, 小威廉絲是上屆...\n",
      "4    [劉秀爲太祖劉邦的九世孫 ， 景皇帝劉啓的七世孫 ， 長沙定王劉發之後 ， 出身於南陽郡的地...\n",
      "Name: evidence_list, dtype: object\n"
     ]
    }
   ],
   "source": [
    "if not TEST_PKL_FILE.exists():\n",
    "    test_df = join_with_topk_evidence(\n",
    "        TEST_DATA_Frame,\n",
    "        mapping,\n",
    "        mode=\"eval\",\n",
    "        topk=EVIDENCE_TOPK,\n",
    "    )\n",
    "    test_df.to_pickle(TEST_PKL_FILE, protocol=4)\n",
    "else:\n",
    "    with open(TEST_PKL_FILE, \"rb\") as f:\n",
    "        test_df = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting evidence_list for the eval mode ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c269d3249a184c0ab2d095b0e3ca1209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=14), Label(value='0 / 14'))), HBox…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "425    [成立於1982年 ， 原爲榮民總醫院臺中分院 ， 後於1988年獨立爲臺中榮民總醫院 。,...\n",
      "441    [《 啓示錄 》 ， 天主教稱 《 若望默示錄 》 （ Αποκάλυψη του Ιωάν...\n",
      "461    [標準的殉爆度測試方法 ， 是取兩段相同的炸藥 ， 分開一個距離 ， 引爆其中一段 ， 看另...\n",
      "552    [穴棲蛇胸鱔爲輻鰭魚綱合鰓目合鰓魚科的其中一種 ， 被IUCN列爲次級保育動物 ， 分佈於中...\n",
      "634    [永生龍 （ 屬名 ： Umoonasaurus ） 是蛇頸龍亞目的一屬 ， 屬於長鎖龍科 ...\n",
      "Name: evidence_list, dtype: object\n"
     ]
    }
   ],
   "source": [
    "test_short_df = join_with_topk_evidence(\n",
    "        TEST_DATA_Short,\n",
    "        mapping,\n",
    "        mode=\"eval\",\n",
    "        topk=EVIDENCE_TOPK,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_short_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILENAME = \"submission.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_short_df_1=test_short_df[test_short_df[\"predicted_evidence\"].map(len)==1]\n",
    "test_short_df_2=test_short_df[test_short_df[\"predicted_evidence\"].map(len)==2]\n",
    "test_short_df_3=test_short_df[test_short_df[\"predicted_evidence\"].map(len)==3]\n",
    "test_short_df_4=test_short_df[test_short_df[\"predicted_evidence\"].map(len)==4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-chinese\")#BertTokenizerFast\n",
    "test_dataset = AicupTopkEvidenceBERT_Test_Dataset(\n",
    "    test_df,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_SEQ_LEN,\n",
    "    topk=EVIDENCE_TOPK\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sub_CKPT_DIR=\"weights/Stage3\"\n",
    "sub_ckpt_name= \"Stage3_model.pt\"\n",
    "model = load_model(model, sub_ckpt_name, Sub_CKPT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2556f054d13b44c2a44473f72811e4ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting:   0%|          | 0/1998 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "predicted_label = run_predict(model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dataset = test_df.copy()\n",
    "predict_dataset[\"predicted_label\"] = list(map(ID2LABEL.get, predicted_label))#row:7990\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f74080d008b4836a931cd2a9afa1e29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset_1 = AicupTopkEvidenceBERT_Test_Dataset(\n",
    "    test_short_df_1,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_SEQ_LEN,\n",
    "    topk=EVIDENCE_TOPK\n",
    ")\n",
    "test_dataloader_1 = DataLoader(test_dataset_1, batch_size=2)\n",
    "predicted_label_1 = run_predict_1(model, test_dataloader_1, device)#row:3\n",
    "predict_dataset_1=test_short_df_1.copy()\n",
    "predict_dataset_1[\"predicted_label\"] = list(map(ID2LABEL.get, predicted_label_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_2 = AicupTopkEvidenceBERT_Test_Dataset(\n",
    "    test_short_df_2,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_SEQ_LEN,\n",
    "    topk=EVIDENCE_TOPK\n",
    ")\n",
    "test_dataloader_2 = DataLoader(test_dataset_2, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e642975e5a03435f86c533737aa41c0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted_label_2 = run_predict_2(model, test_dataloader_2, device)#row:4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dataset_2=test_short_df_2.copy()\n",
    "predict_dataset_2[\"predicted_label\"] = list(map(ID2LABEL.get, predicted_label_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac5bc089c30c4cdda19464a63cb2dfaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset_3 = AicupTopkEvidenceBERT_Test_Dataset(\n",
    "    test_short_df_3,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_SEQ_LEN,\n",
    "    topk=EVIDENCE_TOPK\n",
    ")\n",
    "test_dataloader_3 = DataLoader(test_dataset_3, batch_size=5)\n",
    "predicted_label_3 = run_predict_3(model, test_dataloader_3, device) # 22\n",
    "predict_dataset_3=test_short_df_3.copy()\n",
    "predict_dataset_3[\"predicted_label\"] = list(map(ID2LABEL.get, predicted_label_3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted_label_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f9017c5895454c94fccbb5c318f1b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset_4 = AicupTopkEvidenceBERT_Test_Dataset(\n",
    "    test_short_df_4,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_SEQ_LEN,\n",
    "    topk=EVIDENCE_TOPK\n",
    ")\n",
    "test_dataloader_4 = DataLoader(test_dataset_4, batch_size=5)\n",
    "predicted_label_4 = run_predict_4(model, test_dataloader_4, device) # 27\n",
    "predict_dataset_4=test_short_df_4.copy()\n",
    "predict_dataset_4[\"predicted_label\"] = list(map(ID2LABEL.get, predicted_label_4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted_label_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA_None[\"predicted_evidence\"]=None\n",
    "TEST_DATA_None[\"predicted_label\"]=\"NOT ENOUGH INFO\"\n",
    "predict_dataset_5=TEST_DATA_None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset=pd.concat([predict_dataset,predict_dataset_1,predict_dataset_2,predict_dataset_3,predict_dataset_4,predict_dataset_5])\n",
    "# final_dataset=pd.concat([predict_dataset,predict_dataset_5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predict_dataset_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>claim</th>\n",
       "      <th>predicted_pages</th>\n",
       "      <th>predicted_evidence</th>\n",
       "      <th>evidence_list</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21498</td>\n",
       "      <td>雞形目的鳥腿腳強健，擅長在地面奔跑，其中有珍稀物種，體態雄健優美、顏色鮮豔；也有經濟物種，與...</td>\n",
       "      <td>[鳥, 顏色, 物種, 人類, 雞形目, 關係, 雞]</td>\n",
       "      <td>[[雞形目, 3], [雞形目, 2], [雞形目, 0], [雞形目, 1], [鳥, 0]]</td>\n",
       "      <td>[這一目中的鳥有些體態雄健優美 ， 色彩豔麗 ， 其中不少是珍稀物種和經濟物種 ， 與人類的...</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13037</td>\n",
       "      <td>教會剛建立時為解決內部的一些問題，使徒們寫下許多便條，其中有八卷不是保羅寫的為大公書信。</td>\n",
       "      <td>[書信, 內部, 問題, 大公書信, 保羅·彼特尼, 克里斯·保羅, 教會, 保羅·欣德米特...</td>\n",
       "      <td>[[新約書信, 5], [大公書信, 0], [保羅_(使徒), 3], [大公書信, 4]...</td>\n",
       "      <td>[教會剛成立時爲解決教會里發生的一些問題 ， 使徒 （ 宗徒 ） 們寫下許多書信 ， 這些書...</td>\n",
       "      <td>refutes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18652</td>\n",
       "      <td>羅伯·昆蘭於明尼蘇達州聖保羅市出生。</td>\n",
       "      <td>[羅伯·昆蘭, 明尼蘇達州, 聖保羅市]</td>\n",
       "      <td>[[羅伯·昆蘭, 4], [羅伯·昆蘭, 4], [羅伯·昆蘭, 0], [羅伯·昆蘭, 0...</td>\n",
       "      <td>[1973年3月17日羅伯 · 昆蘭生於明尼蘇達州聖保羅市 。, 1973年3月17日羅伯 ...</td>\n",
       "      <td>supports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21378</td>\n",
       "      <td>2015年美國網球公開賽女子單打比賽裡小威廉絲是上一屆的冠軍。</td>\n",
       "      <td>[2015年, 美國網球公開賽, 蜜雪兒·威廉絲, 2016年法國網球公開賽, 冠軍, 愛莉...</td>\n",
       "      <td>[[2016年法國網球公開賽, 8], [2016年法國網球公開賽, 8], [2015年美...</td>\n",
       "      <td>[小威廉絲是上屆的女子單打冠軍 ， 但她在決賽敗給加比涅 · 穆古魯薩 。, 小威廉絲是上屆...</td>\n",
       "      <td>supports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18624</td>\n",
       "      <td>南陽郡的地方豪族出身的漢光武帝劉秀為太祖劉邦的九世孫。</td>\n",
       "      <td>[漢, 世孫, 太祖, 漢光武帝, 劉邦, 太祖_(祖靈信仰), 南陽郡]</td>\n",
       "      <td>[[漢光武帝, 4], [漢光武帝, 0], [劉邦, 0], [漢光武帝, 6], [南陽...</td>\n",
       "      <td>[劉秀爲太祖劉邦的九世孫 ， 景皇帝劉啓的七世孫 ， 長沙定王劉發之後 ， 出身於南陽郡的地...</td>\n",
       "      <td>supports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6332</th>\n",
       "      <td>17195</td>\n",
       "      <td>土衛十一的命名與泰坦神的無關。</td>\n",
       "      <td>[土衛十一, 土衛十]</td>\n",
       "      <td>[[土衛十一, 0], [土衛十, 4], [土衛十, 0], [土衛十, 1]]</td>\n",
       "      <td>[土衛十一又稱爲 「 艾比米修斯 」 （ Epimetheus ） ， 是土星的一顆內側衛星...</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7222</th>\n",
       "      <td>8370</td>\n",
       "      <td>馬世莉的哥哥因爲心肌梗塞過世。</td>\n",
       "      <td>[馬世莉]</td>\n",
       "      <td>[[馬世莉, 5], [馬世莉, 0], [馬世莉, 4], [馬世莉, 1]]</td>\n",
       "      <td>[因爲身爲家中唯一男丁的哥哥早逝 ， 基於延續馬家香火的想法 ， 將二姐的兒子過繼給未婚的馬...</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>16890</td>\n",
       "      <td>。</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4893</th>\n",
       "      <td>15287</td>\n",
       "      <td>。</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5298</th>\n",
       "      <td>12512</td>\n",
       "      <td>。</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8049 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                              claim  \\\n",
       "0     21498  雞形目的鳥腿腳強健，擅長在地面奔跑，其中有珍稀物種，體態雄健優美、顏色鮮豔；也有經濟物種，與...   \n",
       "1     13037       教會剛建立時為解決內部的一些問題，使徒們寫下許多便條，其中有八卷不是保羅寫的為大公書信。   \n",
       "2     18652                                 羅伯·昆蘭於明尼蘇達州聖保羅市出生。   \n",
       "3     21378                    2015年美國網球公開賽女子單打比賽裡小威廉絲是上一屆的冠軍。   \n",
       "4     18624                        南陽郡的地方豪族出身的漢光武帝劉秀為太祖劉邦的九世孫。   \n",
       "...     ...                                                ...   \n",
       "6332  17195                                    土衛十一的命名與泰坦神的無關。   \n",
       "7222   8370                                    馬世莉的哥哥因爲心肌梗塞過世。   \n",
       "757   16890                                                  。   \n",
       "4893  15287                                                  。   \n",
       "5298  12512                                                  。   \n",
       "\n",
       "                                        predicted_pages  \\\n",
       "0                           [鳥, 顏色, 物種, 人類, 雞形目, 關係, 雞]   \n",
       "1     [書信, 內部, 問題, 大公書信, 保羅·彼特尼, 克里斯·保羅, 教會, 保羅·欣德米特...   \n",
       "2                                  [羅伯·昆蘭, 明尼蘇達州, 聖保羅市]   \n",
       "3     [2015年, 美國網球公開賽, 蜜雪兒·威廉絲, 2016年法國網球公開賽, 冠軍, 愛莉...   \n",
       "4                 [漢, 世孫, 太祖, 漢光武帝, 劉邦, 太祖_(祖靈信仰), 南陽郡]   \n",
       "...                                                 ...   \n",
       "6332                                        [土衛十一, 土衛十]   \n",
       "7222                                              [馬世莉]   \n",
       "757                                                  []   \n",
       "4893                                                 []   \n",
       "5298                                                 []   \n",
       "\n",
       "                                     predicted_evidence  \\\n",
       "0      [[雞形目, 3], [雞形目, 2], [雞形目, 0], [雞形目, 1], [鳥, 0]]   \n",
       "1     [[新約書信, 5], [大公書信, 0], [保羅_(使徒), 3], [大公書信, 4]...   \n",
       "2     [[羅伯·昆蘭, 4], [羅伯·昆蘭, 4], [羅伯·昆蘭, 0], [羅伯·昆蘭, 0...   \n",
       "3     [[2016年法國網球公開賽, 8], [2016年法國網球公開賽, 8], [2015年美...   \n",
       "4     [[漢光武帝, 4], [漢光武帝, 0], [劉邦, 0], [漢光武帝, 6], [南陽...   \n",
       "...                                                 ...   \n",
       "6332          [[土衛十一, 0], [土衛十, 4], [土衛十, 0], [土衛十, 1]]   \n",
       "7222           [[馬世莉, 5], [馬世莉, 0], [馬世莉, 4], [馬世莉, 1]]   \n",
       "757                                                None   \n",
       "4893                                               None   \n",
       "5298                                               None   \n",
       "\n",
       "                                          evidence_list  predicted_label  \n",
       "0     [這一目中的鳥有些體態雄健優美 ， 色彩豔麗 ， 其中不少是珍稀物種和經濟物種 ， 與人類的...  NOT ENOUGH INFO  \n",
       "1     [教會剛成立時爲解決教會里發生的一些問題 ， 使徒 （ 宗徒 ） 們寫下許多書信 ， 這些書...          refutes  \n",
       "2     [1973年3月17日羅伯 · 昆蘭生於明尼蘇達州聖保羅市 。, 1973年3月17日羅伯 ...         supports  \n",
       "3     [小威廉絲是上屆的女子單打冠軍 ， 但她在決賽敗給加比涅 · 穆古魯薩 。, 小威廉絲是上屆...         supports  \n",
       "4     [劉秀爲太祖劉邦的九世孫 ， 景皇帝劉啓的七世孫 ， 長沙定王劉發之後 ， 出身於南陽郡的地...         supports  \n",
       "...                                                 ...              ...  \n",
       "6332  [土衛十一又稱爲 「 艾比米修斯 」 （ Epimetheus ） ， 是土星的一顆內側衛星...  NOT ENOUGH INFO  \n",
       "7222  [因爲身爲家中唯一男丁的哥哥早逝 ， 基於延續馬家香火的想法 ， 將二姐的兒子過繼給未婚的馬...  NOT ENOUGH INFO  \n",
       "757                                                 NaN  NOT ENOUGH INFO  \n",
       "4893                                                NaN  NOT ENOUGH INFO  \n",
       "5298                                                NaN  NOT ENOUGH INFO  \n",
       "\n",
       "[8049 rows x 6 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset[[\"id\", \"predicted_label\", \"predicted_evidence\"]].to_json(\n",
    "    OUTPUT_FILENAME,\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    "    force_ascii=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
