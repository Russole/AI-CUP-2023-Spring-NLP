{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8KjFXiVBfux"
      },
      "outputs": [],
      "source": [
        "# built-in libs\n",
        "import json\n",
        "import pickle\n",
        "import re\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Set, Tuple, Union\n",
        "\n",
        "from helper_function import *\n",
        "# 3rd party libs\n",
        "import hanlp\n",
        "import opencc\n",
        "import pandas as pd\n",
        "import wikipedia\n",
        "from hanlp.components.pipeline import Pipeline\n",
        "from pandarallel import pandarallel\n",
        "# our own libs\n",
        "from utils import load_json\n",
        "#from helper_function import *\n",
        "from utils import (\n",
        "    generate_evidence_to_wiki_pages_mapping,\n",
        "    jsonl_dir_to_df,\n",
        "    load_json,\n",
        ")\n",
        "\n",
        "\n",
        "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=10)\n",
        "wikipedia.set_lang(\"zh\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEAlu9nyBfuz"
      },
      "outputs": [],
      "source": [
        "wiki_pages = jsonl_dir_to_df(\"./data/wiki-pages\")\n",
        "mapping = generate_evidence_to_wiki_pages_mapping(wiki_pages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZFfxtOcBfuz"
      },
      "outputs": [],
      "source": [
        "Private_Data = load_json(\"./data/raw/Private/private_test_data.jsonl\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "k_NCtMmCBfu0"
      },
      "source": [
        "# Hanlp Predictor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DL83H0AjBfu1"
      },
      "outputs": [],
      "source": [
        "predictor = (hanlp.pipeline().append(\n",
        "    hanlp.load(\"FINE_ELECTRA_SMALL_ZH\"),\n",
        "    output_key=\"tok\",\n",
        ").append(\n",
        "    hanlp.load(\"CTB9_CON_ELECTRA_SMALL\"),\n",
        "    output_key=\"con\",\n",
        "    input_key=\"tok\",\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rh2Fq4WfBfu1"
      },
      "outputs": [],
      "source": [
        "hanlp_file = f\"./data/raw/Private/hanlp_con_PrivateTest_results.pkl\"\n",
        "if Path(hanlp_file).exists():\n",
        "    with open(hanlp_file, \"rb\") as f:\n",
        "        hanlp_results = pickle.load(f)\n",
        "else:\n",
        "    hanlp_results = [get_nps_hanlp(predictor, d) for d in Private_Data]\n",
        "    with open(hanlp_file, \"wb\") as f:\n",
        "        pickle.dump(hanlp_results, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJUrjj-ILkUq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "CONVERTER_T2S = opencc.OpenCC(\"t2s.json\")\n",
        "CONVERTER_S2T = opencc.OpenCC(\"s2t.json\")\n",
        "def compare_strings(a, b):\n",
        "    if a is None or b is None:\n",
        "        print(\"Number of Same Characters: 0\")\n",
        "        return\n",
        "    \n",
        "    MaxSize = max(len(a), len(b)) # Finding the max length\n",
        "    MinSize = min(len(a), len(b))\n",
        "    SizeMinus=MaxSize-MinSize\n",
        "    count = 0 # A counter to keep track of same characters\n",
        "\n",
        "    for i in range(MaxSize):\n",
        "        if(i==MinSize):\n",
        "            break\n",
        "        if a[i] != b[i]:\n",
        "            count += 1 # Updating the counter when characters are same at an index\n",
        "    count+=SizeMinus\n",
        "    return count\n",
        "    \n",
        "def do_st_corrections(text: str) -> str:\n",
        "    simplified = CONVERTER_T2S.convert(text)\n",
        "\n",
        "    return CONVERTER_S2T.convert(simplified)\n",
        "def get_nps_hanlp(\n",
        "    predictor: Pipeline,\n",
        "    d\n",
        ") -> List[str]:\n",
        "    claim = d[\"claim\"]\n",
        "    \n",
        "    tree = predictor(claim)[\"con\"][0]\n",
        "    \n",
        "    nps = [\n",
        "        do_st_corrections(\"\".join(subtree.leaves()))\n",
        "        for subtree in tree.subtrees(lambda t: t.label() == \"NP\")\n",
        "    ]\n",
        "    \n",
        "    return nps\n",
        "\n",
        "def TextTok(Text):\n",
        "    Hanlp_Text={}\n",
        "    for title,text in Text.items():\n",
        "        nps_tok={}\n",
        "        if text==\"\":\n",
        "        \n",
        "            nps_tok[\"tok\"]=[]\n",
        "            \n",
        "            Hanlp_Text[title]=nps_tok\n",
        "        else:\n",
        "          try:\n",
        "            tok= predictor(text)[\"tok\"]\n",
        "            nps_tok[\"tok\"]=tok\n",
        "            Hanlp_Text[title]=nps_tok\n",
        "          except RuntimeError as e:\n",
        "              if \"out of memory\" in str(e):\n",
        "                torch.cuda.empty_cache()\n",
        "              nps_tok[\"tok\"]=[]\n",
        "              \n",
        "          Hanlp_Text[title]=nps_tok\n",
        "    Hanlp_TextTok_list=[]\n",
        "    \n",
        "    for k ,v in Hanlp_Text.items():\n",
        "        \n",
        "        Hanlp_TextTok_list.append(v[\"tok\"])\n",
        "    return Hanlp_TextTok_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_GG_atiLXnG"
      },
      "outputs": [],
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "import time\n",
        "import numpy\n",
        "all_candidate_list=[]\n",
        "def get_pred_pages_BM25_ver1(series_data: pd.Series,mapping2) -> Set[Dict[int, str]]: #加上ver3 的 找'_' 的標題\n",
        "    print(series_data.name)\n",
        "    AllCandidateList=[]\n",
        "    candiate_in_claim=[]\n",
        "    nps_in_mapping=[]\n",
        "    nps_in_candiate2=[]\n",
        "    CompareNpsCandidate=[]\n",
        "    \n",
        "    \n",
        "    results = []\n",
        "    tmp_muji = []\n",
        "    # wiki_page: its index showned in claim\n",
        "    mapping = {}\n",
        "    claim = series_data[\"claim\"]\n",
        "    nps = series_data[\"hanlp_results\"]\n",
        "    \n",
        "    first_wiki_term = [] \n",
        "    for np in nps:\n",
        "      \n",
        "      if np in mapping2:\n",
        "        nps_in_mapping.append(np)\n",
        "    \n",
        "    if len(nps)==0:\n",
        "      return set([]) \n",
        "    for i, np in enumerate(nps):\n",
        "        # Simplified Traditional Chinese Correction\n",
        "        wiki_search_results = [\n",
        "            do_st_corrections(w) for w in wikipedia.search(np)\n",
        "        ]\n",
        "\n",
        "        # Remove the wiki page's description in brackets\n",
        "        wiki_set = [re.sub(r\"\\s\\(\\S+\\)\", \"\", w) for w in wiki_search_results]\n",
        "\n",
        "        wiki_set = [w for w in wiki_search_results]#璿鈞改的(ver 1)\n",
        "\n",
        "        wiki_df = pd.DataFrame({\n",
        "            \"wiki_set\": wiki_set,\n",
        "            \"wiki_results\": wiki_search_results\n",
        "        })\n",
        "\n",
        "        # Elements in wiki_set --> index\n",
        "        # Extracting only the first element is one way to avoid extracting\n",
        "        # too many of the similar wiki pages\n",
        "        grouped_df = wiki_df.groupby(\"wiki_set\", sort=False).first()\n",
        "        candidates = grouped_df[\"wiki_results\"].tolist()\n",
        "        all_candidate_list.append(candidates)\n",
        "\n",
        "        AllCandidateList.extend(candidates)#連上透過np search到的candidates\n",
        "\n",
        "        # muji refers to wiki_set\n",
        "        muji = grouped_df.index.tolist()\n",
        "\n",
        "        for candidate in candidates:\n",
        "          if candidate in claim:\n",
        "              \n",
        "              candiate_in_claim.append(candidate)\n",
        "\n",
        "        for prefix, term in zip(muji, candidates):\n",
        "            if prefix not in tmp_muji:\n",
        "                matched = False\n",
        "\n",
        "                # Take at least one term from the first noun phrase\n",
        "                if i == 0:\n",
        "                    first_wiki_term.append(term)\n",
        "\n",
        "                # Walrus operator :=\n",
        "                # https://docs.python.org/3/whatsnew/3.8.html#assignment-expressions\n",
        "                # Through these filters, we are trying to figure out if the term\n",
        "                # is within the claim\n",
        "                if (((new_term := term) in claim) or\n",
        "                    ((new_term := term.replace(\"·\", \"\")) in claim) or\n",
        "                    ((new_term := term.split(\" \")[0]) in claim) or\n",
        "                    ((new_term := term.replace(\"-\", \" \")) in claim)):\n",
        "                    matched = True\n",
        "\n",
        "                elif \"·\" in term:\n",
        "                    splitted = term.split(\"·\")\n",
        "                    for split in splitted:\n",
        "                        if (new_term := split) in claim:\n",
        "                            matched = True\n",
        "                            break\n",
        "\n",
        "                if matched:\n",
        "                    # post-processing\n",
        "                    term = term.replace(\" \", \"_\")\n",
        "                    term = term.replace(\"-\", \"\")\n",
        "                    results.append(term)\n",
        "                    mapping[term] = claim.find(new_term)\n",
        "                    tmp_muji.append(new_term)\n",
        "        #time.sleep(0.5)\n",
        "    ######### BM 25 ###################################\n",
        "    if len(AllCandidateList) == 0:\n",
        "      return set([])\n",
        "    Test_Doc_Full=list(set(AllCandidateList))\n",
        "    Text={}\n",
        "    for Doc in Test_Doc_Full:\n",
        "        TempNull=wiki_pages[wiki_pages[\"id\"]==Doc].empty\n",
        "        if not TempNull:\n",
        "            Text[Doc]=wiki_pages[wiki_pages[\"id\"]==Doc].values[0][1]\n",
        "        else:\n",
        "            Text[Doc]=\"\"\n",
        "    \n",
        "    Hanlp_TextTok_list=TextTok(Text)\n",
        "    Text_bm25 = BM25Okapi(Hanlp_TextTok_list)\n",
        "\n",
        "    Claim_tok = predictor(claim)[\"tok\"]\n",
        "    doc_scores = Text_bm25.get_scores(Claim_tok)\n",
        "    \n",
        "    BM25_Answer_Index=numpy.argsort(doc_scores)[-1]\n",
        "    \n",
        "    BM25_Answer=[Test_Doc_Full[BM25_Answer_Index]]\n",
        "    ######### BM 25 Finish###################################    \n",
        "\n",
        "    # 5 is a hyperparameter\n",
        "    if len(results) > 10:\n",
        "        assert -1 not in mapping.values()\n",
        "        results = sorted(mapping, key=mapping.get)[:10]\n",
        "    elif len(results) < 1:\n",
        "        results = first_wiki_term\n",
        "    results.extend(candiate_in_claim)\n",
        "    results.extend(nps_in_mapping)\n",
        "    results.extend(BM25_Answer)\n",
        "    \n",
        "    return set(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "yPs1ZkHMBfu1",
        "outputId": "0c2de427-79ac-4f31-cea6-2a437ab8aa43"
      },
      "outputs": [],
      "source": [
        "doc_path = f\"./data/raw/Private/Private_doc10_BM25_final.jsonl\"\n",
        "if Path(doc_path).exists():\n",
        "    with open(doc_path, \"r\", encoding=\"utf8\") as f:\n",
        "        predicted_results = pd.Series([\n",
        "            set(json.loads(line)[\"predicted_pages\"])\n",
        "            for line in f\n",
        "        ])\n",
        "else:\n",
        "    private_df = pd.DataFrame(Private_Data)\n",
        "    private_df.loc[:, \"hanlp_results\"] = hanlp_results\n",
        "    predicted_results = private_df.apply(get_pred_pages_BM25_ver1, mapping2 = mapping, axis=1)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "re3yceQ_f0t0"
      },
      "outputs": [],
      "source": [
        "def save_private_doc(\n",
        "    data: List[Dict[str, Union[int, Claim, Evidence]]],\n",
        "    predictions: pd.Series,\n",
        "    mode: str = \"train\",\n",
        "    num_pred_doc: int = 5,\n",
        "    \n",
        ") -> None:\n",
        "    with open(\n",
        "        f\"./data/Stage1/{mode}_doc{num_pred_doc}_BM25_Ver1_final.jsonl\",\n",
        "        \"w\",\n",
        "        encoding=\"utf8\",\n",
        "    ) as f:\n",
        "        for i, d in enumerate(data):\n",
        "            print(i)\n",
        "            d[\"predicted_pages\"] = list(predictions.iloc[i])\n",
        "            f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNF948sQLPVo"
      },
      "outputs": [],
      "source": [
        "save_private_doc(Private_Data, predicted_results, mode=\"Private\",num_pred_doc=10)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
