{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "r8KjFXiVBfux"
      },
      "outputs": [],
      "source": [
        "# built-in libs\n",
        "import json\n",
        "import pickle\n",
        "import re\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Set, Tuple, Union\n",
        "\n",
        "from helper_function import *\n",
        "# 3rd party libs\n",
        "import hanlp\n",
        "import opencc\n",
        "import pandas as pd\n",
        "import wikipedia\n",
        "from hanlp.components.pipeline import Pipeline\n",
        "from pandarallel import pandarallel\n",
        "# our own libs\n",
        "from utils import load_json\n",
        "#from helper_function import *\n",
        "from utils import (\n",
        "    generate_evidence_to_wiki_pages_mapping,\n",
        "    jsonl_dir_to_df,\n",
        "    load_json,\n",
        ")\n",
        "\n",
        "\n",
        "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=10)\n",
        "wikipedia.set_lang(\"zh\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AEAlu9nyBfuz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading and concatenating jsonl files in ./data/wiki-pages\n",
            "Generate parse mapping\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b69516b3550149628a82d45dfc491c38",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=118776), Label(value='0 / 118776')…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transform to id to evidence_map mapping\n"
          ]
        }
      ],
      "source": [
        "wiki_pages = jsonl_dir_to_df(\"./data/wiki-pages\")\n",
        "mapping = generate_evidence_to_wiki_pages_mapping(wiki_pages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5ZFfxtOcBfuz"
      },
      "outputs": [],
      "source": [
        "Private_Data = load_json(\"./data/raw/Private/private_test_data.jsonl\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "k_NCtMmCBfu0"
      },
      "source": [
        "# Hanlp Predictor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DL83H0AjBfu1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                             \r"
          ]
        }
      ],
      "source": [
        "predictor = (hanlp.pipeline().append(\n",
        "    hanlp.load(\"FINE_ELECTRA_SMALL_ZH\"),\n",
        "    output_key=\"tok\",\n",
        ").append(\n",
        "    hanlp.load(\"CTB9_CON_ELECTRA_SMALL\"),\n",
        "    output_key=\"con\",\n",
        "    input_key=\"tok\",\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Rh2Fq4WfBfu1"
      },
      "outputs": [],
      "source": [
        "hanlp_file = f\"./data/raw/Private/hanlp_con_PrivateTest_results.pkl\"\n",
        "if Path(hanlp_file).exists():\n",
        "    with open(hanlp_file, \"rb\") as f:\n",
        "        hanlp_results = pickle.load(f)\n",
        "else:\n",
        "    hanlp_results = [get_nps_hanlp(predictor, d) for d in Private_Data]\n",
        "    with open(hanlp_file, \"wb\") as f:\n",
        "        pickle.dump(hanlp_results, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cJUrjj-ILkUq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "CONVERTER_T2S = opencc.OpenCC(\"t2s.json\")\n",
        "CONVERTER_S2T = opencc.OpenCC(\"s2t.json\")\n",
        "def compare_strings(a, b):\n",
        "    if a is None or b is None:\n",
        "        print(\"Number of Same Characters: 0\")\n",
        "        return\n",
        "    \n",
        "    MaxSize = max(len(a), len(b)) # Finding the max length\n",
        "    MinSize = min(len(a), len(b))\n",
        "    SizeMinus=MaxSize-MinSize\n",
        "    count = 0 # A counter to keep track of same characters\n",
        "\n",
        "    for i in range(MaxSize):\n",
        "        if(i==MinSize):\n",
        "            break\n",
        "        if a[i] != b[i]:\n",
        "            count += 1 # Updating the counter when characters are same at an index\n",
        "    count+=SizeMinus\n",
        "    return count\n",
        "    \n",
        "def do_st_corrections(text: str) -> str:\n",
        "    simplified = CONVERTER_T2S.convert(text)\n",
        "\n",
        "    return CONVERTER_S2T.convert(simplified)\n",
        "def get_nps_hanlp(\n",
        "    predictor: Pipeline,\n",
        "    d\n",
        ") -> List[str]:\n",
        "    claim = d[\"claim\"]\n",
        "    \n",
        "    tree = predictor(claim)[\"con\"][0]\n",
        "    \n",
        "    nps = [\n",
        "        do_st_corrections(\"\".join(subtree.leaves()))\n",
        "        for subtree in tree.subtrees(lambda t: t.label() == \"NP\")\n",
        "    ]\n",
        "    \n",
        "    return nps\n",
        "\n",
        "def TextTok(Text):\n",
        "    Hanlp_Text={}\n",
        "    for title,text in Text.items():\n",
        "        nps_tok={}\n",
        "        if text==\"\":\n",
        "        \n",
        "            nps_tok[\"tok\"]=[]\n",
        "            \n",
        "            Hanlp_Text[title]=nps_tok\n",
        "        else:\n",
        "          try:\n",
        "            tok= predictor(text)[\"tok\"]\n",
        "            nps_tok[\"tok\"]=tok\n",
        "            Hanlp_Text[title]=nps_tok\n",
        "          except RuntimeError as e:\n",
        "              if \"out of memory\" in str(e):\n",
        "                torch.cuda.empty_cache()\n",
        "              nps_tok[\"tok\"]=[]\n",
        "              \n",
        "          Hanlp_Text[title]=nps_tok\n",
        "    Hanlp_TextTok_list=[]\n",
        "    \n",
        "    for k ,v in Hanlp_Text.items():\n",
        "        \n",
        "        Hanlp_TextTok_list.append(v[\"tok\"])\n",
        "    return Hanlp_TextTok_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0_GG_atiLXnG"
      },
      "outputs": [],
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "import time\n",
        "import numpy\n",
        "all_candidate_list=[]\n",
        "def get_pred_pages_BM25_ver1(series_data: pd.Series,mapping2) -> Set[Dict[int, str]]: #加上ver3 的 找'_' 的標題\n",
        "    print(series_data.name)\n",
        "    AllCandidateList=[]\n",
        "    candiate_in_claim=[]\n",
        "    nps_in_mapping=[]\n",
        "    nps_in_candiate2=[]\n",
        "    CompareNpsCandidate=[]\n",
        "    \n",
        "    \n",
        "    results = []\n",
        "    tmp_muji = []\n",
        "    # wiki_page: its index showned in claim\n",
        "    mapping = {}\n",
        "    claim = series_data[\"claim\"]\n",
        "    nps = series_data[\"hanlp_results\"]\n",
        "    \n",
        "    first_wiki_term = [] \n",
        "    for np in nps:\n",
        "      \n",
        "      if np in mapping2:\n",
        "        nps_in_mapping.append(np)\n",
        "    \n",
        "    if len(nps)==0:\n",
        "      return set([]) \n",
        "    for i, np in enumerate(nps):\n",
        "        # Simplified Traditional Chinese Correction\n",
        "        wiki_search_results = [\n",
        "            do_st_corrections(w) for w in wikipedia.search(np)\n",
        "        ]\n",
        "\n",
        "        # Remove the wiki page's description in brackets\n",
        "        wiki_set = [re.sub(r\"\\s\\(\\S+\\)\", \"\", w) for w in wiki_search_results]\n",
        "\n",
        "        wiki_set = [w for w in wiki_search_results]#璿鈞改的(ver 1)\n",
        "\n",
        "        wiki_df = pd.DataFrame({\n",
        "            \"wiki_set\": wiki_set,\n",
        "            \"wiki_results\": wiki_search_results\n",
        "        })\n",
        "\n",
        "        # Elements in wiki_set --> index\n",
        "        # Extracting only the first element is one way to avoid extracting\n",
        "        # too many of the similar wiki pages\n",
        "        grouped_df = wiki_df.groupby(\"wiki_set\", sort=False).first()\n",
        "        candidates = grouped_df[\"wiki_results\"].tolist()\n",
        "        all_candidate_list.append(candidates)\n",
        "\n",
        "        AllCandidateList.extend(candidates)#連上透過np search到的candidates\n",
        "\n",
        "        # muji refers to wiki_set\n",
        "        muji = grouped_df.index.tolist()\n",
        "\n",
        "        for candidate in candidates:\n",
        "          if candidate in claim:\n",
        "              \n",
        "              candiate_in_claim.append(candidate)\n",
        "\n",
        "        for prefix, term in zip(muji, candidates):\n",
        "            if prefix not in tmp_muji:\n",
        "                matched = False\n",
        "\n",
        "                # Take at least one term from the first noun phrase\n",
        "                if i == 0:\n",
        "                    first_wiki_term.append(term)\n",
        "\n",
        "                # Walrus operator :=\n",
        "                # https://docs.python.org/3/whatsnew/3.8.html#assignment-expressions\n",
        "                # Through these filters, we are trying to figure out if the term\n",
        "                # is within the claim\n",
        "                if (((new_term := term) in claim) or\n",
        "                    ((new_term := term.replace(\"·\", \"\")) in claim) or\n",
        "                    ((new_term := term.split(\" \")[0]) in claim) or\n",
        "                    ((new_term := term.replace(\"-\", \" \")) in claim)):\n",
        "                    matched = True\n",
        "\n",
        "                elif \"·\" in term:\n",
        "                    splitted = term.split(\"·\")\n",
        "                    for split in splitted:\n",
        "                        if (new_term := split) in claim:\n",
        "                            matched = True\n",
        "                            break\n",
        "\n",
        "                if matched:\n",
        "                    # post-processing\n",
        "                    term = term.replace(\" \", \"_\")\n",
        "                    term = term.replace(\"-\", \"\")\n",
        "                    results.append(term)\n",
        "                    mapping[term] = claim.find(new_term)\n",
        "                    tmp_muji.append(new_term)\n",
        "        #time.sleep(0.5)\n",
        "    ######### BM 25 ###################################\n",
        "    if len(AllCandidateList) == 0:\n",
        "      return set([])\n",
        "    Test_Doc_Full=list(set(AllCandidateList))\n",
        "    Text={}\n",
        "    for Doc in Test_Doc_Full:\n",
        "        TempNull=wiki_pages[wiki_pages[\"id\"]==Doc].empty\n",
        "        if not TempNull:\n",
        "            Text[Doc]=wiki_pages[wiki_pages[\"id\"]==Doc].values[0][1]\n",
        "        else:\n",
        "            Text[Doc]=\"\"\n",
        "    \n",
        "    Hanlp_TextTok_list=TextTok(Text)\n",
        "    Text_bm25 = BM25Okapi(Hanlp_TextTok_list)\n",
        "\n",
        "    Claim_tok = predictor(claim)[\"tok\"]\n",
        "    doc_scores = Text_bm25.get_scores(Claim_tok)\n",
        "    \n",
        "    BM25_Answer_Index=numpy.argsort(doc_scores)[-1]\n",
        "    \n",
        "    BM25_Answer=[Test_Doc_Full[BM25_Answer_Index]]\n",
        "    ######### BM 25 Finish###################################    \n",
        "\n",
        "    # 5 is a hyperparameter\n",
        "    if len(results) > 10:\n",
        "        assert -1 not in mapping.values()\n",
        "        results = sorted(mapping, key=mapping.get)[:10]\n",
        "    elif len(results) < 1:\n",
        "        results = first_wiki_term\n",
        "    results.extend(candiate_in_claim)\n",
        "    results.extend(nps_in_mapping)\n",
        "    results.extend(BM25_Answer)\n",
        "    \n",
        "    return set(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "yPs1ZkHMBfu1",
        "outputId": "0c2de427-79ac-4f31-cea6-2a437ab8aa43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n"
          ]
        }
      ],
      "source": [
        "doc_path = f\"./data/Stage1/Private_doc10_BM25_ver1_final.jsonl\"\n",
        "if Path(doc_path).exists():\n",
        "    with open(doc_path, \"r\", encoding=\"utf8\") as f:\n",
        "        predicted_results = pd.Series([\n",
        "            set(json.loads(line)[\"predicted_pages\"])\n",
        "            for line in f\n",
        "        ])\n",
        "else:\n",
        "    private_df = pd.DataFrame(Private_Data)\n",
        "    private_df.loc[:, \"hanlp_results\"] = hanlp_results\n",
        "    predicted_results = private_df.apply(get_pred_pages_BM25_ver1, mapping2 = mapping, axis=1)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "re3yceQ_f0t0"
      },
      "outputs": [],
      "source": [
        "def save_private_doc(\n",
        "    data: List[Dict[str, Union[int, Claim, Evidence]]],\n",
        "    predictions: pd.Series,\n",
        "    mode: str = \"train\",\n",
        "    num_pred_doc: int = 5,\n",
        "    \n",
        ") -> None:\n",
        "    with open(\n",
        "        f\"./data/Stage1/{mode}_doc{num_pred_doc}_BM25_ver1_final.jsonl\",\n",
        "        \"w\",\n",
        "        encoding=\"utf8\",\n",
        "    ) as f:\n",
        "        for i, d in enumerate(data):\n",
        "            print(i)\n",
        "            d[\"predicted_pages\"] = list(predictions.iloc[i])\n",
        "            f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sNF948sQLPVo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n"
          ]
        }
      ],
      "source": [
        "save_private_doc(Private_Data, predicted_results, mode=\"Private\",num_pred_doc=10)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
