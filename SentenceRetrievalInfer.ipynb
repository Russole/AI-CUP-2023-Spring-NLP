{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# built-in libs\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Set, Tuple, Union\n",
    "\n",
    "# 3rd party libs\n",
    "import hanlp\n",
    "import opencc\n",
    "import pandas as pd\n",
    "import wikipedia\n",
    "from hanlp.components.pipeline import Pipeline\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "# our own libs\n",
    "from utils import load_json\n",
    "\n",
    "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=10)\n",
    "wikipedia.set_lang(\"zh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# built-in libs\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Set, Tuple, Union\n",
    "\n",
    "# third-party libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    get_scheduler,BertTokenizerFast\n",
    ")\n",
    "\n",
    "from dataset import BERTDataset, Dataset\n",
    "\n",
    "# local libs\n",
    "from utils import (\n",
    "    generate_evidence_to_wiki_pages_mapping,\n",
    "    jsonl_dir_to_df,\n",
    "    load_json,\n",
    "    load_model,\n",
    "    save_checkpoint,\n",
    "    set_lr_scheduler,\n",
    ")\n",
    "\n",
    "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and concatenating jsonl files in data/wiki-pages\n",
      "Generate parse mapping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bceda0e5acf475db677054000e3b51f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=118776), Label(value='0 / 118776')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform to id to evidence_map mapping\n"
     ]
    }
   ],
   "source": [
    "wiki_pages = jsonl_dir_to_df(\"data/wiki-pages\")\n",
    "mapping = generate_evidence_to_wiki_pages_mapping(wiki_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evidence_macro_precision(\n",
    "    instance: Dict,\n",
    "    top_rows: pd.DataFrame,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Calculate precision for sentence retrieval\n",
    "    This function is modified from fever-scorer.\n",
    "    https://github.com/sheffieldnlp/fever-scorer/blob/master/src/fever/scorer.py\n",
    "\n",
    "    Args:\n",
    "        instance (dict): a row of the dev set (dev.jsonl) of test set (test.jsonl)\n",
    "        top_rows (pd.DataFrame): our predictions with the top probabilities\n",
    "\n",
    "        IMPORTANT!!!\n",
    "        instance (dict) should have the key of `evidence`.\n",
    "        top_rows (pd.DataFrame) should have a column `predicted_evidence`.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]:\n",
    "        [1]: relevant and retrieved (numerator of precision)\n",
    "        [2]: retrieved (denominator of precision)\n",
    "    \"\"\"\n",
    "    this_precision = 0.0\n",
    "    this_precision_hits = 0.0\n",
    "\n",
    "    # Return 0, 0 if label is not enough info since not enough info does not\n",
    "    # contain any evidence.\n",
    "    if instance[\"label\"].upper() != \"NOT ENOUGH INFO\":\n",
    "        # e[2] is the page title, e[3] is the sentence index\n",
    "        all_evi = [[e[2], e[3]]\n",
    "                   for eg in instance[\"evidence\"]\n",
    "                   for e in eg\n",
    "                   if e[3] is not None]\n",
    "        claim = instance[\"claim\"]\n",
    "        predicted_evidence = top_rows[top_rows[\"claim\"] ==\n",
    "                                      claim][\"predicted_evidence\"].tolist()\n",
    "\n",
    "        for prediction in predicted_evidence:\n",
    "            if prediction in all_evi:\n",
    "                this_precision += 1.0\n",
    "            this_precision_hits += 1.0\n",
    "\n",
    "        return (this_precision /\n",
    "                this_precision_hits) if this_precision_hits > 0 else 1.0, 1.0\n",
    "\n",
    "    return 0.0, 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evidence_macro_recall(\n",
    "    instance: Dict,\n",
    "    top_rows: pd.DataFrame,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Calculate recall for sentence retrieval\n",
    "    This function is modified from fever-scorer.\n",
    "    https://github.com/sheffieldnlp/fever-scorer/blob/master/src/fever/scorer.py\n",
    "\n",
    "    Args:\n",
    "        instance (dict): a row of the dev set (dev.jsonl) of test set (test.jsonl)\n",
    "        top_rows (pd.DataFrame): our predictions with the top probabilities\n",
    "\n",
    "        IMPORTANT!!!\n",
    "        instance (dict) should have the key of `evidence`.\n",
    "        top_rows (pd.DataFrame) should have a column `predicted_evidence`.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]:\n",
    "        [1]: relevant and retrieved (numerator of recall)\n",
    "        [2]: relevant (denominator of recall)\n",
    "    \"\"\"\n",
    "    # We only want to score F1/Precision/Recall of recalled evidence for NEI claims\n",
    "    if instance[\"label\"].upper() != \"NOT ENOUGH INFO\":\n",
    "        # If there's no evidence to predict, return 1\n",
    "        if len(instance[\"evidence\"]) == 0 or all(\n",
    "            [len(eg) == 0 for eg in instance]):\n",
    "            return 1.0, 1.0\n",
    "\n",
    "        claim = instance[\"claim\"]\n",
    "\n",
    "        predicted_evidence = top_rows[top_rows[\"claim\"] ==\n",
    "                                      claim][\"predicted_evidence\"].tolist()\n",
    "\n",
    "        for evidence_group in instance[\"evidence\"]:\n",
    "            evidence = [[e[2], e[3]] for e in evidence_group]\n",
    "\n",
    "            if all([item in predicted_evidence for item in evidence]):\n",
    "            # if any([item in predicted_evidence for item in evidence]):\n",
    "                #print([item in predicted_evidence for item in evidence])\n",
    "                # We only want to score complete groups of evidence. Incomplete\n",
    "                # groups are worthless.\n",
    "                return 1.0, 1.0\n",
    "        return 0.0, 1.0\n",
    "    return 0.0, 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval(\n",
    "    probs: np.ndarray,\n",
    "    df_evidences: pd.DataFrame,\n",
    "    ground_truths: pd.DataFrame,\n",
    "    top_n: int = 5,\n",
    "    cal_scores: bool = True,\n",
    "    save_name: str = None,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Calculate the scores of sentence retrieval\n",
    "\n",
    "    Args:\n",
    "        probs (np.ndarray): probabilities of the candidate retrieved sentences\n",
    "        df_evidences (pd.DataFrame): the candiate evidence sentences paired with claims\n",
    "        ground_truths (pd.DataFrame): the loaded data of dev.jsonl or test.jsonl\n",
    "        top_n (int, optional): the number of the retrieved sentences. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: F1 score, precision, and recall\n",
    "    \"\"\"\n",
    "    df_evidences[\"prob\"] = probs\n",
    "    top_rows = (\n",
    "        df_evidences.groupby(\"claim\").apply(\n",
    "        lambda x: x.nlargest(top_n, \"prob\"))\n",
    "        .reset_index(drop=True)\n",
    "    )#前5名機率高的句子。\n",
    "    #print(top_rows.loc[0:6])\n",
    "    if cal_scores:\n",
    "        macro_precision = 0\n",
    "        macro_precision_hits = 0\n",
    "        macro_recall = 0\n",
    "        macro_recall_hits = 0\n",
    "\n",
    "        for i, instance in enumerate(ground_truths):\n",
    "            #print(instance)\n",
    "            macro_prec = evidence_macro_precision(instance, top_rows)\n",
    "            macro_precision += macro_prec[0]\n",
    "            macro_precision_hits += macro_prec[1]\n",
    "\n",
    "            macro_rec = evidence_macro_recall(instance, top_rows)\n",
    "            macro_recall += macro_rec[0]\n",
    "            macro_recall_hits += macro_rec[1]\n",
    "\n",
    "        pr = (macro_precision /\n",
    "              macro_precision_hits) if macro_precision_hits > 0 else 1.0\n",
    "        rec = (macro_recall /\n",
    "               macro_recall_hits) if macro_recall_hits > 0 else 0.0\n",
    "        f1 = 2.0 * pr * rec / (pr + rec)\n",
    "\n",
    "    if save_name is not None:\n",
    "        # write doc7_sent5 file\n",
    "        # with open(f\"data/{save_name}\", \"w\") as f:\n",
    "        with open(f\"{save_name}\", \"w\") as f:\n",
    "            for instance in ground_truths:\n",
    "                claim = instance[\"claim\"]\n",
    "                predicted_evidence = top_rows[\n",
    "                    top_rows[\"claim\"] == claim][\"predicted_evidence\"].tolist()\n",
    "                instance[\"predicted_evidence\"] = predicted_evidence\n",
    "                f.write(json.dumps(instance, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    if cal_scores:\n",
    "        return {\"F1 score\": f1, \"Precision\": pr, \"Recall\": rec}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_probs(\n",
    "    model: nn.Module,\n",
    "    dataloader: Dataset,\n",
    "    device: torch.device,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Inference script to get probabilites for the candidate evidence sentences\n",
    "\n",
    "    Args:\n",
    "        model: the one from HuggingFace Transformers\n",
    "        dataloader: devset or testset in torch dataloader\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: probabilites of the candidate evidence sentences\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits#logits shape:(256,2)\n",
    "            \n",
    "            #print(torch.softmax(logits, dim=1).shape)\n",
    "            probs.extend(torch.softmax(logits, dim=1)[:, 1].tolist())#對每列進行softmax，torch.softmax(logits, dim=1) shape:(256,2)\n",
    "\n",
    "    return np.array(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentRetrievalBERTDataset(BERTDataset):\n",
    "    \"\"\"AicupTopkEvidenceBERTDataset class for AICUP dataset with top-k evidence sentences.\"\"\"\n",
    "\n",
    "    def __getitem__(\n",
    "        self,\n",
    "        idx: int,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[Dict[str, torch.Tensor], int]:\n",
    "        item = self.data.iloc[idx]\n",
    "        sentA = item[\"claim\"]\n",
    "        sentB = item[\"text\"]\n",
    "\n",
    "        # claim [SEP] text\n",
    "        concat = self.tokenizer(\n",
    "            sentA,\n",
    "            sentB,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "        )\n",
    "        concat_ten = {k: torch.tensor(v) for k, v in concat.items()}\n",
    "        if \"label\" in item:\n",
    "            concat_ten[\"labels\"] = torch.tensor(item[\"label\"])\n",
    "\n",
    "        return concat_ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_with_wiki_sentences(\n",
    "    mapping: Dict[str, Dict[int, str]],\n",
    "    df: pd.DataFrame,\n",
    "    negative_ratio: float,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Only for creating train sentences.\"\"\"\n",
    "    claims = []\n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    # positive\n",
    "    for i in range(len(df)):\n",
    "        if df[\"label\"].iloc[i] == \"NOT ENOUGH INFO\":\n",
    "            continue\n",
    "\n",
    "        claim = df[\"claim\"].iloc[i]\n",
    "        evidence_sets = df[\"evidence\"].iloc[i]\n",
    "        for evidence_set in evidence_sets:\n",
    "            sents = []\n",
    "            for evidence in evidence_set:\n",
    "                \n",
    "                # evidence[2] is the page title\n",
    "                page = evidence[2].replace(\" \", \"_\")\n",
    "                # the only page with weird name\n",
    "                if page == \"臺灣海峽危機#第二次臺灣海峽危機（1958）\":\n",
    "                    continue\n",
    "                # evidence[3] is in form of int however, mapping requires str\n",
    "                sent_idx = str(evidence[3])\n",
    "                sents.append(mapping[page][sent_idx])\n",
    "\n",
    "            whole_evidence = \" \".join(sents)\n",
    "\n",
    "            claims.append(claim)\n",
    "            sentences.append(whole_evidence)\n",
    "            labels.append(1)\n",
    "\n",
    "    # negative\n",
    "    for i in range(len(df)):\n",
    "        if df[\"label\"].iloc[i] == \"NOT ENOUGH INFO\":\n",
    "            continue\n",
    "        claim = df[\"claim\"].iloc[i]\n",
    "\n",
    "        evidence_set = set([(evidence[2], evidence[3])\n",
    "                            for evidences in df[\"evidence\"][i]\n",
    "                            for evidence in evidences])\n",
    "        predicted_pages = df[\"predicted_pages\"][i]\n",
    "        for page in predicted_pages:\n",
    "            page = page.replace(\" \", \"_\")\n",
    "            try:\n",
    "                page_sent_id_pairs = [\n",
    "                    (page, sent_idx) for sent_idx in mapping[page].keys()\n",
    "                ]\n",
    "                \n",
    "            except KeyError:\n",
    "                # print(f\"{page} is not in our Wiki db.\")\n",
    "                continue\n",
    "\n",
    "            for pair in page_sent_id_pairs:\n",
    "                if pair in evidence_set:\n",
    "                    continue\n",
    "                text = mapping[page][pair[1]]\n",
    "                # `np.random.rand(1) <= 0.05`: Control not to add too many negative samples\n",
    "                if text != \"\" and np.random.rand(1) <= negative_ratio:\n",
    "                    claims.append(claim)\n",
    "                    sentences.append(text)\n",
    "                    labels.append(0)\n",
    "\n",
    "    return pd.DataFrame({\"claim\": claims, \"text\": sentences, \"label\": labels})\n",
    "\n",
    "\n",
    "def pair_with_wiki_sentences_eval(\n",
    "    mapping: Dict[str, Dict[int, str]],\n",
    "    df: pd.DataFrame,\n",
    "    is_testset: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Only for creating dev and test sentences.\"\"\"\n",
    "    claims = []\n",
    "    sentences = []\n",
    "    evidence = []\n",
    "    predicted_evidence = []\n",
    "\n",
    "    # negative\n",
    "    for i in range(len(df)):\n",
    "        # if df[\"label\"].iloc[i] == \"NOT ENOUGH INFO\":\n",
    "        #     continue\n",
    "        claim = df[\"claim\"].iloc[i]\n",
    "\n",
    "        predicted_pages = df[\"predicted_pages\"][i]\n",
    "        for page in predicted_pages:\n",
    "            page = page.replace(\" \", \"_\")\n",
    "            try:\n",
    "                page_sent_id_pairs = [(page, k) for k in mapping[page]]\n",
    "            except KeyError:\n",
    "                # print(f\"{page} is not in our Wiki db.\")\n",
    "                continue\n",
    "\n",
    "            for page_name, sentence_id in page_sent_id_pairs:\n",
    "                text = mapping[page][sentence_id]\n",
    "                if text != \"\":\n",
    "                    claims.append(claim)\n",
    "                    sentences.append(text)\n",
    "                    if not is_testset:\n",
    "                        evidence.append(df[\"evidence\"].iloc[i])\n",
    "                    predicted_evidence.append([page_name, int(sentence_id)])\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"claim\": claims,\n",
    "        \"text\": sentences,\n",
    "        \"evidence\": evidence if not is_testset else None,\n",
    "        \"predicted_evidence\": predicted_evidence,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title  { display-mode: \"form\" }\n",
    "\n",
    "MODEL_NAME = \"bert-base-chinese\"  #@param {type:\"string\"}\n",
    "TEST_BATCH_SIZE = 256  #@param {type:\"integer\"}\n",
    "TOP_N = 5  #@param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME) \n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") \n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME) \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sub_CKPT_DIR=\"weights/Stage2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start final evaluations and write prediction files.\n"
     ]
    }
   ],
   "source": [
    "sub_name = \"model.525.pt\"\n",
    "model = load_model(model, sub_name, Sub_CKPT_DIR)\n",
    "print(\"Start final evaluations and write prediction files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = load_json(\"data/Stage1/Private_doc10_BM25_ver1_final.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predicting the test data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10bb08c9665248aa825b3f658b9b16fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1525 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#test_data = load_json(\"data/SplitTestData/test_doc10_BM25(ver2).jsonl\")#C:\\Users\\Lab000\\Desktop\\competition\\NLP\\NCKU_2023_baseline\\ProposedMethod\\data\\SplitTestData\n",
    "\n",
    "test_evidences = pair_with_wiki_sentences_eval(\n",
    "    mapping,\n",
    "    pd.DataFrame(test_data),\n",
    "    is_testset=True,\n",
    ")\n",
    "test_set = SentRetrievalBERTDataset(test_evidences, tokenizer)\n",
    "test_dataloader = DataLoader(test_set, batch_size=TEST_BATCH_SIZE)\n",
    "\n",
    "print(\"Start predicting the test data\")\n",
    "probs = get_predicted_probs(model, test_dataloader, device)\n",
    "evaluate_retrieval(\n",
    "    probs=probs,\n",
    "    df_evidences=test_evidences,\n",
    "    ground_truths=test_data,\n",
    "    top_n=TOP_N,\n",
    "    cal_scores=False,\n",
    "    save_name=f\"data/Stage2/Private/Private_doc10sent{TOP_N}_BM25_ver1_final.jsonl\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "contrastive_learning2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
